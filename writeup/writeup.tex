\documentclass[psamsfonts]{amsart}
\usepackage[h margin=1 in, v margin=1 in]{geometry}
%-------Packages---------
\usepackage{amssymb,amsfonts}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{yfonts}
\usepackage{enumitem}
\usepackage{mathrsfs}
\usepackage{fourier-orns}
\usepackage[all]{xy}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{url}
\usepackage{mathtools}

\usepackage{tgpagella}
\usepackage[T1]{fontenc}

%--------Theorem Environments--------
%theoremstyle{plain} --- default
\newtheorem{thm}{Theorem}[section]
\newtheorem*{thm*}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem*{lem*}{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{quest}[thm]{Question}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem*{defn*}{Definition}
\newtheorem{defns}[thm]{Definitions}
\newtheorem{con}[thm]{Construction}
\newtheorem{exmp}[thm]{Example}
\newtheorem{exmps}[thm]{Examples}
\newtheorem{notn}[thm]{Notation}
\newtheorem{notns}[thm]{Notations}
\newtheorem{addm}[thm]{Addendum}
\newtheorem{exer}[thm]{Exercise}


\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem*{claim}{Claim}
\newtheorem*{rem*}{Remark}
\newtheorem*{hint*}{Hint}
\newtheorem*{note}{Note}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{warn}[thm]{Warning}
\newtheorem{sch}[thm]{Scholium}
\renewcommand{\qedsymbol}{$\blacksquare$}
\renewcommand{\emptyset}{\varnothing}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\p}{\mathbb{P}}
\newcommand{\V}{\vec{v}}
\newcommand{\RP}{\mathbb{R}\mathbf{P}}
\newcommand{\CP}{\mathbb{C}\mathbf{P}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\GL}{\mathsf{GL}}
\newcommand{\SL}{\mathsf{SL}}
\newcommand{\SP}{\mathsf{SP}}
\newcommand{\SO}{\mathsf{SO}}
\newcommand{\SU}{\mathsf{SU}}
\newcommand{\gl}{\mathfrak{gl}}
\newcommand{\g}{\mathfrak{g}}

\newtheorem{prob}{Problem}[section]

\newcommand{\inv}{^{-1}}
\newcommand{\bra}[2]{ \left[ #1, #2 \right] }
\newcommand{\ind}{\lambda \in \Lambda}
\newcommand{\set}[1]{\left\lbrace#1 \right\rbrace}
\newcommand{\imp}[2]{ \underline{ #1 \implies #2} }
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\transv}{\mathrel{\text{\tpitchfork}}}
\let\oldexists\exists
\renewcommand\exists{\oldexists~}
\let\oldL\L
\renewcommand\L{\mathfrak{L}}
\makeatletter
\newcommand{\tpitchfork}{%
  \vbox{
    \baselineskip\z@skip
    \lineskip-.52ex
    \lineskiplimit\maxdimen
    \m@th
    \ialign{##\crcr\hidewidth\smash{$-$}\hidewidth\crcr$\pitchfork$\crcr}
  }%
}
\makeatother

\newcommand{\bd}{\partial}

\newcommand{\lang}{\begin{picture}(5,7)
\put(1.1,2.5){\rotatebox{45}{\line(1,0){6.0}}}
\put(1.1,2.5){\rotatebox{315}{\line(1,0){6.0}}}
\end{picture}}
\newcommand{\rang}{\begin{picture}(5,7)
\put(.1,2.5){\rotatebox{135}{\line(1,0){6.0}}}
\put(.1,2.5){\rotatebox{225}{\line(1,0){6.0}}}
\end{picture}}


\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\grap}{graph}
\DeclareMathOperator{\codim}{codim}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\inter}{Int}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\indx}{ind}
\DeclareMathOperator{\alt}{Alt}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\trace}{trace}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Ad}{Ad}
\DeclareMathOperator{\Lie}{Lie}




\newcommand*\myhrulefill{%
   \leavevmode\leaders\hrule depth-2pt height 2.4pt\hfill\kern0pt}

\newcommand\niceending[1]{%
  \begin{center}%
    \LARGE \myhrulefill \hspace{0.2cm} #1 \hspace{0.2cm} \myhrulefill%
  \end{center}}

\newcommand*\sectionend{\niceending{\decofourleft\decofourright}}
\newcommand*\subsectionend{\niceending{\decosix}}


\def\upint{\mathchoice%
    {\mkern13mu\overline{\vphantom{\intop}\mkern7mu}\mkern-20mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
  \int}
\def\lowint{\mkern3mu\underline{\vphantom{\intop}\mkern7mu}\mkern-10mu\int}

\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=blue,
    urlcolor=black
}

\newenvironment{solution}
  {\begin{proof}[Solution]}
  {\end{proof}}


\setcounter{section}{0}
\begin{document}
\author{Jeffrey Jiang, Kurtis David, Quang Duong}
\title{CS342 Final Project Writeup}
\maketitle

\section*{Racing Strategy}

We decided that there were several things that could be hard coded, in order to simplify the task of our neural network. The hard coded things did not need to be outputs of our network. When playing the game, we take the action output the network and add on several values.
\begin{enumerate}
\item Our kart will always accelerate. Therefore, we always add 4 to the output of our network
\item Items aren't particularly important in learning how to drive. Therefore, we also hard coded for items to be used immediately when acquired. We also use the boost whenever it is acquired as well.
\item Using the state variable given by the function step, we can retrieve the position\_along\_track value, which we used to check if our agent was idling. If it idled too long, we hard coded a rescue.
\end{enumerate}

We also decided that self-supervised learning was going to be much worse than if we could fully supervise training, so we chose to do imitation learning.


\section*{Data collection}

In order to do imitation learning, we have to have a dataset recorded from something we want our agent to imitate. One option was to record one of us playing the game, but we decided to hard code a basic AI to play the game and record its actions for use in our dataset. The gist of the AI's strategy is:
\begin{enumerate}
\item The AI always accelerates
\item The AI attempts to minimize the state variable distance\_to\_center, which we use in conjunction with the angle variable to determine whether to turn left or right. Our reasoning is that our ideal player would simply follow the center of the track at all times, accelerating the entire time.
\end{enumerate}


 We chose to go with the AI for several reasons
\begin{enumerate}
\item Player inputs aren't very clean. For example, there are several steps where we pressed both left and right at the same time, and some times where we didn't press any buttons. This makes player input pretty unpredictable. In contrast, an AI has extremely predictable behavior, which makes the classification task much easier for the network.
\item The AI has a very well defined goal in terms of the state, which again should make the agent easier to train, since we give the state as input. Therefore, it should be relatively straightforward to predict the AI's action.
\end{enumerate}

To record the data, we simply recorded the action our AI output, as well as the state of the game at the time step to numpy arrays and saved them. Some of the variables did not seem very useful, so we omitted them in the recording, we chose position\_along\_track, distance\_to\_center, speed, angle, and smooth\_speed,  as our input variables to be recorded. 

In addition, we found that collecting images and using them as input to harmful, as they added too many additional variables into our network, when the state already provides all the information we wanted for our policy. As another note, our basic AI does not do any image processing, so making the input to the network the same as the variables used by the AI's policy is quite reasonable.

\section*{Network Architecture}

\section*{What we tried}
Some other things we tried
\begin{enumerate}
\item \textbf{Gradient Free: }Our first idea was to use a gradient free method similar to HW10, trying both a genetic algorithm and a hill climbing algorithm. However, the results we had were not particularly good. We felt like the task was too complex to learn in a gradient free manner. In addition, evaluating the fitness value of a set of weights would take a long time, since it would have to play the game for several seconds. Therefore, optimization would take a long time, unless we limited ourselves to a very small amount of samples and a low amount of training steps
\item \textbf{Q-Learning: } We also tried a self-supervised method. Again, we found that the task is probably too complex to learn without supervision, and our Q-Learning algorithm didn't produce very good results. Even though we thought there was a relatively natural reward function to optimize (distance to center), the agent we trained in the manner did not perform very well. 
\item \textbf{Fully Connected: } We also tried a very straightforward classifcation network that we trained by taking states and feeding it through 5 fully connected layers, with the AI's output as the labels. This worked quite well in replicating the AI, but we decided that LSTM's were a more interesting architechture.
\end{enumerate}

One particular thing to noe with gradient free and Q-learning is that both are heavily reliant on a reward/fitness function, which is quite difficult to define, since it's hard to quantify how well you are driving. Also, reward functions can have unexpected behavior which is optimal for the reward, but is not desirable for actually playing the game.


\end{document}